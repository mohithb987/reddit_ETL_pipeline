[2024-06-05T12:09:35.904+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-05T12:09:35.919+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: reddit_etl_pipeline.reddit_extraction manual__2024-06-05T12:09:35.121137+00:00 [queued]>
[2024-06-05T12:09:35.923+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: reddit_etl_pipeline.reddit_extraction manual__2024-06-05T12:09:35.121137+00:00 [queued]>
[2024-06-05T12:09:35.924+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-05T12:09:35.930+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): reddit_extraction> on 2024-06-05 12:09:35.121137+00:00
[2024-06-05T12:09:35.933+0000] {standard_task_runner.py:63} INFO - Started process 57 to run task
[2024-06-05T12:09:35.937+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'reddit_etl_pipeline', 'reddit_extraction', 'manual__2024-06-05T12:09:35.121137+00:00', '--job-id', '8', '--raw', '--subdir', 'DAGS_FOLDER/reddit_dag.py', '--cfg-path', '/tmp/tmpa3m4ciry']
[2024-06-05T12:09:35.942+0000] {standard_task_runner.py:91} INFO - Job 8: Subtask reddit_extraction
[2024-06-05T12:09:35.986+0000] {task_command.py:426} INFO - Running <TaskInstance: reddit_etl_pipeline.reddit_extraction manual__2024-06-05T12:09:35.121137+00:00 [running]> on host 59ea7148f4e9
[2024-06-05T12:09:36.029+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Mohith' AIRFLOW_CTX_DAG_ID='reddit_etl_pipeline' AIRFLOW_CTX_TASK_ID='reddit_extraction' AIRFLOW_CTX_EXECUTION_DATE='2024-06-05T12:09:35.121137+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-06-05T12:09:35.121137+00:00'
[2024-06-05T12:09:36.030+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-05T12:09:36.039+0000] {logging_mixin.py:188} INFO - Connected to Reddit!
[2024-06-05T12:09:36.714+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'https://www.databricks.com/blog/databricks-tabular', 'author_fullname': 't2_cqao8', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Databricks acquires Tabular', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d8118g', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.98, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 173, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 173, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717517504.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p><a href="https://www.databricks.com/blog/databricks-tabular">https://www.databricks.com/blog/databricks-tabular</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/AIrGfRgf_b_kylATL05Oolsfw7zRvONIXqMyffTxckA.jpg?auto=webp&s=3d1dbb9eb4567b595b09a2c4ab79e99733275803', 'width': 1200, 'height': 628}, 'resolutions': [{'url': 'https://external-preview.redd.it/AIrGfRgf_b_kylATL05Oolsfw7zRvONIXqMyffTxckA.jpg?width=108&crop=smart&auto=webp&s=b84644c4d6543a2adbc8d9be1014afd9c757041e', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/AIrGfRgf_b_kylATL05Oolsfw7zRvONIXqMyffTxckA.jpg?width=216&crop=smart&auto=webp&s=1e33c8b6a6cf151a1ac2a7c6f036efcb225e66b8', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/AIrGfRgf_b_kylATL05Oolsfw7zRvONIXqMyffTxckA.jpg?width=320&crop=smart&auto=webp&s=065a2897d2a50f6eda15b6fb4b342dabe6506c1d', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/AIrGfRgf_b_kylATL05Oolsfw7zRvONIXqMyffTxckA.jpg?width=640&crop=smart&auto=webp&s=df4a3ebb584e3a4ba07dea46b026c356b155607c', 'width': 640, 'height': 334}, {'url': 'https://external-preview.redd.it/AIrGfRgf_b_kylATL05Oolsfw7zRvONIXqMyffTxckA.jpg?width=960&crop=smart&auto=webp&s=10bd7882f338cab807d7f974aad4bc2720377cc0', 'width': 960, 'height': 502}, {'url': 'https://external-preview.redd.it/AIrGfRgf_b_kylATL05Oolsfw7zRvONIXqMyffTxckA.jpg?width=1080&crop=smart&auto=webp&s=64bf31b836d0c8f3e1b81db6a565e942e4f2debd', 'width': 1080, 'height': 565}], 'variants': {}, 'id': 'N152Gho5EKv55Wck64WuBIBCDKz1itMsLbzYZu2zeyM'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1d8118g', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='dan_the_lion'), 'discussion_type': None, 'num_comments': 93, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d8118g/databricks_acquires_tabular/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d8118g/databricks_acquires_tabular/', 'subreddit_subscribers': 188236, 'created_utc': 1717517504.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.715+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Every DE job I’ve ever had is understaffed and over taxed with high pressure management! Is this the nature of the profession or have I just been unlucky?\n\nI specialize in AWS and SparkSQL for reference. ', 'author_fullname': 't2_6gmtyh4l', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How to find chill jobs?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d7yfij', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.93, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 82, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 82, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717511041.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Every DE job I’ve ever had is understaffed and over taxed with high pressure management! Is this the nature of the profession or have I just been unlucky?</p>\n\n<p>I specialize in AWS and SparkSQL for reference. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1d7yfij', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='MimiTheWitch'), 'discussion_type': None, 'num_comments': 64, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d7yfij/how_to_find_chill_jobs/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d7yfij/how_to_find_chill_jobs/', 'subreddit_subscribers': 188236, 'created_utc': 1717511041.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.715+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "With Tabular's acquisition by Databricks [today](https://www.wsj.com/articles/databricks-to-buy-data-management-startup-tabular-in-bid-for-ai-clients-829e5bcf), I thought it would be a good time to reflect on Apache Iceberg's position in light of today's events.\n\nTwo weeks ago I attended the Iceberg conference and was amazed at how energized it was. I wrote the following 4 points in reference to Iceberg: \n\n--------\n\n1. Apache Iceberg is being adopted by some of the largest companies on the planet, including Netflix, Apple, and Google in various ways and in various projects. Each of these organizations is actively following developments in the Apache Iceberg open source community. \n\n2. Iceberg means different things for different people. One company might get added benefit in AWS S3 costs, or compute costs. Another might benefit from features like time travel. It's the combination of these attributes that is pushing Iceberg forward because it basically makes sense for everyone. \n\n3. Iceberg is changing fast and what we have now won't be the finished state in the future. For example, Puffin files can be used to develop better query plans and improve query execution. \n\n4. Openness helps everyone and in one way or another. Everyone was talking about the benefits of avoiding vendor lock in and retaining options. \n\n------\n\nKnowing what we know now, how do people think the announcements by both Snowflake (Polaris) and Databricks (Tabular acquisition) will change anything for Iceberg? \n\nWill all of the points above still remain valid? Will it open up a new debate regarding Iceberg implementations vs the table formats themselves?", 'author_fullname': 't2_p25cwdu4', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': "What's next for Apache Iceberg? ", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d89flp', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.96, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 55, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 55, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717538295.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>With Tabular&#39;s acquisition by Databricks <a href="https://www.wsj.com/articles/databricks-to-buy-data-management-startup-tabular-in-bid-for-ai-clients-829e5bcf">today</a>, I thought it would be a good time to reflect on Apache Iceberg&#39;s position in light of today&#39;s events.</p>\n\n<p>Two weeks ago I attended the Iceberg conference and was amazed at how energized it was. I wrote the following 4 points in reference to Iceberg: </p>\n\n<hr/>\n\n<ol>\n<li><p>Apache Iceberg is being adopted by some of the largest companies on the planet, including Netflix, Apple, and Google in various ways and in various projects. Each of these organizations is actively following developments in the Apache Iceberg open source community. </p></li>\n<li><p>Iceberg means different things for different people. One company might get added benefit in AWS S3 costs, or compute costs. Another might benefit from features like time travel. It&#39;s the combination of these attributes that is pushing Iceberg forward because it basically makes sense for everyone. </p></li>\n<li><p>Iceberg is changing fast and what we have now won&#39;t be the finished state in the future. For example, Puffin files can be used to develop better query plans and improve query execution. </p></li>\n<li><p>Openness helps everyone and in one way or another. Everyone was talking about the benefits of avoiding vendor lock in and retaining options. </p></li>\n</ol>\n\n<hr/>\n\n<p>Knowing what we know now, how do people think the announcements by both Snowflake (Polaris) and Databricks (Tabular acquisition) will change anything for Iceberg? </p>\n\n<p>Will all of the points above still remain valid? Will it open up a new debate regarding Iceberg implementations vs the table formats themselves?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1d89flp', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Teach-To-The-Tech'), 'discussion_type': None, 'num_comments': 36, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d89flp/whats_next_for_apache_iceberg/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d89flp/whats_next_for_apache_iceberg/', 'subreddit_subscribers': 188236, 'created_utc': 1717538295.0, 'num_crossposts': 1, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.716+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'My colleagues and I have been working on making Dask fast. It’s been fun. Dask DataFrame is now 20x faster and \\~50% faster than Spark (but it depends a lot on the workload).\n\n\n\nI wrote a blog post on what we did: [https://docs.coiled.io/blog/dask-dataframe-is-fast.html](https://docs.coiled.io/blog/dask-dataframe-is-fast.html)\n\n\n\nReally, this came down not to doing one thing really well, but doing lots of small things “pretty good”. Some of the most prominent changes include:\n\n1. Apache Arrow support in pandas\n2. Better shuffling algorithm for faster joins\n3. Automatic query optimization\n\n\n\nThere are a bunch of other improvements too like copy-on-write for pandas 2.0 which ensures copies are only triggered when necessary, GIL fixes in pandas, better serialization, a new parquet reader, etc. We were able to get a 20x speedup on traditional DataFrame benchmarks.\n\n\n\nI’d love it if people tried things out or suggested improvements we might have overlooked.\n\nBlog post: [https://docs.coiled.io/blog/dask-dataframe-is-fast.html](https://docs.coiled.io/blog/dask-dataframe-is-fast.html)', 'author_fullname': 't2_o3c8q92a', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Dask DataFrame is Fast Now!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d7w2px', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.91, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 54, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 54, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717504585.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>My colleagues and I have been working on making Dask fast. It’s been fun. Dask DataFrame is now 20x faster and ~50% faster than Spark (but it depends a lot on the workload).</p>\n\n<p>I wrote a blog post on what we did: <a href="https://docs.coiled.io/blog/dask-dataframe-is-fast.html">https://docs.coiled.io/blog/dask-dataframe-is-fast.html</a></p>\n\n<p>Really, this came down not to doing one thing really well, but doing lots of small things “pretty good”. Some of the most prominent changes include:</p>\n\n<ol>\n<li>Apache Arrow support in pandas</li>\n<li>Better shuffling algorithm for faster joins</li>\n<li>Automatic query optimization</li>\n</ol>\n\n<p>There are a bunch of other improvements too like copy-on-write for pandas 2.0 which ensures copies are only triggered when necessary, GIL fixes in pandas, better serialization, a new parquet reader, etc. We were able to get a 20x speedup on traditional DataFrame benchmarks.</p>\n\n<p>I’d love it if people tried things out or suggested improvements we might have overlooked.</p>\n\n<p>Blog post: <a href="https://docs.coiled.io/blog/dask-dataframe-is-fast.html">https://docs.coiled.io/blog/dask-dataframe-is-fast.html</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1d7w2px', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='phofl93'), 'discussion_type': None, 'num_comments': 25, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d7w2px/dask_dataframe_is_fast_now/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d7w2px/dask_dataframe_is_fast_now/', 'subreddit_subscribers': 188236, 'created_utc': 1717504585.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.717+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "TL;DR: Sqlfluff rewritten in Rust, about 10x speed improvement and portable\n\nhttps://github.com/quarylabs/sqruff\n\nAt [Quary](https://www.quary.dev/), we're big fans of [SQLFluff](https://sqlfluff.com/)! It's the most comprehensive formatter/linter about! It outputs great-looking code and has great checks for writing high-quality SQL.\n\nThat said, it can often be slow, and in some CI pipelines we've seen it be the slowest step. To help us and our customers, we decided to rewrite it in Rust to get faster performance and portability to be able to run it anywhere.\n\nSqruff currently supports the following dialects: ANSI, BigQuery, Postgres and we are working on the next  Snowflake and Clickhouse next.\n\nIn terms of performance, we tend to see about 10x speed improvement for a single file when run in the sqruff repo:\n\n```\n    time sqruff lint crates/lib/test/fixtures/dialects/ansi/drop_index_if_exists.sql\n    0.01s user 0.01s system 42% cpu 0.041 total\n            \n    time sqlfluff lint crates/lib/test/fixtures/dialects/ansi/drop_index_if_exists.sql\n    0.23s user 0.06s system 74% cpu 0.398 total\n```\n\nAnd for a whole list of files, we see about 9x improvement depending on what you measure:\n\n```\n    time sqruff lint crates/lib/test/fixtures/dialects/ansi    \n    4.23s user 1.53s system 735% cpu 0.784 total\n        \n    time sqlfluff lint crates/lib/test/fixtures/dialects/ansi\n    5.44s user 0.43s system 93% cpu 6.312 total\n```\n\nBoth above were run on an M1 Mac.", 'author_fullname': 't2_q5jcx79', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Fast open-source SQL formatter/linter: Sqruff', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d7y932', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.94, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 30, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 30, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717510579.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>TL;DR: Sqlfluff rewritten in Rust, about 10x speed improvement and portable</p>\n\n<p><a href="https://github.com/quarylabs/sqruff">https://github.com/quarylabs/sqruff</a></p>\n\n<p>At <a href="https://www.quary.dev/">Quary</a>, we&#39;re big fans of <a href="https://sqlfluff.com/">SQLFluff</a>! It&#39;s the most comprehensive formatter/linter about! It outputs great-looking code and has great checks for writing high-quality SQL.</p>\n\n<p>That said, it can often be slow, and in some CI pipelines we&#39;ve seen it be the slowest step. To help us and our customers, we decided to rewrite it in Rust to get faster performance and portability to be able to run it anywhere.</p>\n\n<p>Sqruff currently supports the following dialects: ANSI, BigQuery, Postgres and we are working on the next  Snowflake and Clickhouse next.</p>\n\n<p>In terms of performance, we tend to see about 10x speed improvement for a single file when run in the sqruff repo:</p>\n\n<p>```\n    time sqruff lint crates/lib/test/fixtures/dialects/ansi/drop_index_if_exists.sql\n    0.01s user 0.01s system 42% cpu 0.041 total</p>\n\n<pre><code>time sqlfluff lint crates/lib/test/fixtures/dialects/ansi/drop_index_if_exists.sql\n0.23s user 0.06s system 74% cpu 0.398 total\n</code></pre>\n\n<p>```</p>\n\n<p>And for a whole list of files, we see about 9x improvement depending on what you measure:</p>\n\n<p>```\n    time sqruff lint crates/lib/test/fixtures/dialects/ansi<br/>\n    4.23s user 1.53s system 735% cpu 0.784 total</p>\n\n<pre><code>time sqlfluff lint crates/lib/test/fixtures/dialects/ansi\n5.44s user 0.43s system 93% cpu 6.312 total\n</code></pre>\n\n<p>```</p>\n\n<p>Both above were run on an M1 Mac.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?auto=webp&s=87a0b3113d8a8e8a4fad2901b55e98449c340d4a', 'width': 2048, 'height': 2048}, 'resolutions': [{'url': 'https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=108&crop=smart&auto=webp&s=e09230469f5ffa3e0cd2651e18d860042ba4e457', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=216&crop=smart&auto=webp&s=95f7839ea5bdb3e5e564f2e5afa389d6990bfb89', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=320&crop=smart&auto=webp&s=fda65169a18d5a214619e0464afdb2852107fa47', 'width': 320, 'height': 320}, {'url': 'https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=640&crop=smart&auto=webp&s=57c64448d9e4256e89bfc92a1e03627f1e1c4050', 'width': 640, 'height': 640}, {'url': 'https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=960&crop=smart&auto=webp&s=4cde8788c99759e9bf6399fa46456d0e851962e6', 'width': 960, 'height': 960}, {'url': 'https://external-preview.redd.it/A1vwNwLQxTn2aelQrmAIMxQsZn6FVTPz_BpEgMA5b8o.jpg?width=1080&crop=smart&auto=webp&s=9c224395a06778cde7bd43e5c28b6fab324950f9', 'width': 1080, 'height': 1080}], 'variants': {}, 'id': 'yi1H-2cV1KrS4mf3uf9SP2lDbruuU_3Qx50MFRPXxz4'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1d7y932', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='bk1007'), 'discussion_type': None, 'num_comments': 17, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d7y932/fast_opensource_sql_formatterlinter_sqruff/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d7y932/fast_opensource_sql_formatterlinter_sqruff/', 'subreddit_subscribers': 188236, 'created_utc': 1717510579.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.718+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm a Senior DE and I work in a team dedicated to a single big client of the company I work for. Recently have been asked to to take the reins of the technological evolution, so it will be about partecipating at strategic meetings, work with the client, the team and the team's manager when it comes to new projects, reduce costs, introduce new and change existing infra/worklfow.\n\nI believe this what a tech lead does and I would like your opinions on:  \n- What you do as a tech lead  \n- What you think you should really do   \n- How do you bring value\n\nThanks a lot :)", 'author_fullname': 't2_d0ifg2cb', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What do you do as a tech lead in Data Engineering?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d8k2lm', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.92, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 18, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 18, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717572481.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m a Senior DE and I work in a team dedicated to a single big client of the company I work for. Recently have been asked to to take the reins of the technological evolution, so it will be about partecipating at strategic meetings, work with the client, the team and the team&#39;s manager when it comes to new projects, reduce costs, introduce new and change existing infra/worklfow.</p>\n\n<p>I believe this what a tech lead does and I would like your opinions on:<br/>\n- What you do as a tech lead<br/>\n- What you think you should really do<br/>\n- How do you bring value</p>\n\n<p>Thanks a lot :)</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1d8k2lm', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Awkward-Cupcake6219'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d8k2lm/what_do_you_do_as_a_tech_lead_in_data_engineering/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d8k2lm/what_do_you_do_as_a_tech_lead_in_data_engineering/', 'subreddit_subscribers': 188236, 'created_utc': 1717572481.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.719+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm at a crossroad in my career.  I'm making about $200K, but dread going in to work every day.\n\nMy company has decades of tech debt that I've been trying to clean up, but it's only there because there's also decades culture that put it there.  To some degree, that's part of the job, so I thought I was prepared for it.\n\nWrong.\n\nWhile I enjoy fixing bad code and improving all things data related, I'm finding out just how toxic this place can be.  I'm the 3rd or maybe 4th engineer in my role within 1 year.  My boss, fired.  We're extremely short staffed, so burnout is real.  Project management?  More like project micromanagement.  Only, they can't decide if they're waterfall or agile, so they blame it on the tooling system (Jira) and are talking about hopping platforms.  Budgets?  Nope.  Make a proposal, but you won't get written approval.  Instead,  a handslap when you go over... \n\nThe list goes on, but what it boils down to is the company is a game of thrones.  Rather than fixing what's broke, every few months there's a round of finger pointing, demotions, firing, or people just walk.  The people who stay seem to always be competing for each other's jobs and constantly trying to make each other look bad.\n\nAt this point, I'm ready to walk.  The challenge is, I'm so burnt out, I need to take a break before hopping right into another job.  I'm honestly just scared to take that time off because of how crappy the job market is right now and would have a hard time walking away from the money.  But at this point, every day sucks.\n\nCurious what you guys would do?", 'author_fullname': 't2_a3u15uyc', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Mental Health vs Pay', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d8gax2', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.79, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 13, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 13, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717558187.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m at a crossroad in my career.  I&#39;m making about $200K, but dread going in to work every day.</p>\n\n<p>My company has decades of tech debt that I&#39;ve been trying to clean up, but it&#39;s only there because there&#39;s also decades culture that put it there.  To some degree, that&#39;s part of the job, so I thought I was prepared for it.</p>\n\n<p>Wrong.</p>\n\n<p>While I enjoy fixing bad code and improving all things data related, I&#39;m finding out just how toxic this place can be.  I&#39;m the 3rd or maybe 4th engineer in my role within 1 year.  My boss, fired.  We&#39;re extremely short staffed, so burnout is real.  Project management?  More like project micromanagement.  Only, they can&#39;t decide if they&#39;re waterfall or agile, so they blame it on the tooling system (Jira) and are talking about hopping platforms.  Budgets?  Nope.  Make a proposal, but you won&#39;t get written approval.  Instead,  a handslap when you go over... </p>\n\n<p>The list goes on, but what it boils down to is the company is a game of thrones.  Rather than fixing what&#39;s broke, every few months there&#39;s a round of finger pointing, demotions, firing, or people just walk.  The people who stay seem to always be competing for each other&#39;s jobs and constantly trying to make each other look bad.</p>\n\n<p>At this point, I&#39;m ready to walk.  The challenge is, I&#39;m so burnt out, I need to take a break before hopping right into another job.  I&#39;m honestly just scared to take that time off because of how crappy the job market is right now and would have a hard time walking away from the money.  But at this point, every day sucks.</p>\n\n<p>Curious what you guys would do?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1d8gax2', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Lower_Sun_7354'), 'discussion_type': None, 'num_comments': 8, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d8gax2/mental_health_vs_pay/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d8gax2/mental_health_vs_pay/', 'subreddit_subscribers': 188236, 'created_utc': 1717558187.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.719+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I am working as a Low-Code No-Code developer for more than 2 years. I want to move to data engineering roles.\n\nHow should I go about it?\n\nEDIT:\n\nI am a software engineer based out of India. I am currently learning about Apache Airflow, dbt, data warehousing concepts along with Python and SQL. If anyone has gone through a similar phase, I'd love to hear from you!", 'author_fullname': 't2_a8qys001c', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How do I switch to DE from low-code-no-code development?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d8276g', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 12, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 12, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1717556293.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717520446.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I am working as a Low-Code No-Code developer for more than 2 years. I want to move to data engineering roles.</p>\n\n<p>How should I go about it?</p>\n\n<p>EDIT:</p>\n\n<p>I am a software engineer based out of India. I am currently learning about Apache Airflow, dbt, data warehousing concepts along with Python and SQL. If anyone has gone through a similar phase, I&#39;d love to hear from you!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1d8276g', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='No-Story-7786'), 'discussion_type': None, 'num_comments': 9, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d8276g/how_do_i_switch_to_de_from_lowcodenocode/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d8276g/how_do_i_switch_to_de_from_lowcodenocode/', 'subreddit_subscribers': 188236, 'created_utc': 1717520446.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.720+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "***As a data engineer, what concepts do you find most important?***  \n  \n That is, the concepts that are a little further away from the area itself.  \n I'm 18 years old and I've been studying to enter the field for two years, but my total focus has been improving my skills in:  \n  \n1. Python  \n2. Pandas  \n3. PySpark   \n4. SQL   \n5. Cloud (AWS)  \n  \nUnderstanding fundamentals (distributed computing, architecture, modeling), (network concepts, containers/VMs, CLI, Git and a little bit of low-level programming).  \n  \n But I'm not very familiar with back-end development, for example. If someone asked me to do a simple CRUD, it would be a challenge for me. Do you have any advice? I feel like this could somehow be a problem in the future. And if there's something that doesn't get talked about often, but is really important to have in your toolbox.\n\n*I'm trying to optimize my studies and be able to focus on the really important concepts, I live alone and work, so I only have 6 hours a day to study and I still need a lot of optimization for them to really happen.*  \n\n\n**OBS**: Sorry for the English, I'm not native :/ .", 'author_fullname': 't2_b1h6cmbz', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Concepts beyond data engineering|', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d7z8w4', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.84, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 12, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 12, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717513126.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p><strong><em>As a data engineer, what concepts do you find most important?</em></strong>  </p>\n\n<p>That is, the concepts that are a little further away from the area itself.<br/>\n I&#39;m 18 years old and I&#39;ve been studying to enter the field for two years, but my total focus has been improving my skills in:  </p>\n\n<ol>\n<li>Python<br/></li>\n<li>Pandas<br/></li>\n<li>PySpark<br/></li>\n<li>SQL<br/></li>\n<li>Cloud (AWS)<br/></li>\n</ol>\n\n<p>Understanding fundamentals (distributed computing, architecture, modeling), (network concepts, containers/VMs, CLI, Git and a little bit of low-level programming).  </p>\n\n<p>But I&#39;m not very familiar with back-end development, for example. If someone asked me to do a simple CRUD, it would be a challenge for me. Do you have any advice? I feel like this could somehow be a problem in the future. And if there&#39;s something that doesn&#39;t get talked about often, but is really important to have in your toolbox.</p>\n\n<p><em>I&#39;m trying to optimize my studies and be able to focus on the really important concepts, I live alone and work, so I only have 6 hours a day to study and I still need a lot of optimization for them to really happen.</em>  </p>\n\n<p><strong>OBS</strong>: Sorry for the English, I&#39;m not native :/ .</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1d7z8w4', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Original_Transition2'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d7z8w4/concepts_beyond_data_engineering/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d7z8w4/concepts_beyond_data_engineering/', 'subreddit_subscribers': 188236, 'created_utc': 1717513126.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.721+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Given the recent acquisition of Tabular and Snowflakes announcement I’m getting even more curious about Iceberg.\nWhat makes Iceberg good and what problem does it solve?\nWhen it is a good solution in a data stack?', 'author_fullname': 't2_xrfs4dir1', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Why Apache Iceberg?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d8kl5i', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.91, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 9, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 9, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717574772.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Given the recent acquisition of Tabular and Snowflakes announcement I’m getting even more curious about Iceberg.\nWhat makes Iceberg good and what problem does it solve?\nWhen it is a good solution in a data stack?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1d8kl5i', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='StandardDeviationist'), 'discussion_type': None, 'num_comments': 9, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d8kl5i/why_apache_iceberg/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d8kl5i/why_apache_iceberg/', 'subreddit_subscribers': 188236, 'created_utc': 1717574772.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.721+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hey I'm trying to transition into Data Engineeing field from an eLearning Developer. I've couple years of experience working with SQL and Python.\n\nI'm looking to work on some projects to add to my portfolio. Do you recommend any websites for those ideas?", 'author_fullname': 't2_yoget', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Projects for Data Engineering Practice?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d8fn43', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.89, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 7, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 7, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717556016.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey I&#39;m trying to transition into Data Engineeing field from an eLearning Developer. I&#39;ve couple years of experience working with SQL and Python.</p>\n\n<p>I&#39;m looking to work on some projects to add to my portfolio. Do you recommend any websites for those ideas?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1d8fn43', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Drrazor'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d8fn43/projects_for_data_engineering_practice/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d8fn43/projects_for_data_engineering_practice/', 'subreddit_subscribers': 188236, 'created_utc': 1717556016.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.722+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "My background:\n\n- 5 years of python experience (no formal CS education)\n\n- did the DE Zoomcamp and applied most of it to my (soon to be over) current job, (Had a mirror DB in BigQuery synced to our prod database with a script ran in GCP and created several BI friendly tables from that, infra handled with Terraform).\n\n- many soft skills (originally a Product Manager, lead many B2B contacts in the fintech industry)\n\nWhere should I start looking for a job? What should I pay attention to? Any tips on what to expect? I've seen many Senior postings but none for entry level (I guess that's me?)", 'author_fullname': 't2_4rpxclpr', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': "I'm about to quit my job and start my journey on hunting for my first DE job. Any tips?", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d8bgdh', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.69, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717543537.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>My background:</p>\n\n<ul>\n<li><p>5 years of python experience (no formal CS education)</p></li>\n<li><p>did the DE Zoomcamp and applied most of it to my (soon to be over) current job, (Had a mirror DB in BigQuery synced to our prod database with a script ran in GCP and created several BI friendly tables from that, infra handled with Terraform).</p></li>\n<li><p>many soft skills (originally a Product Manager, lead many B2B contacts in the fintech industry)</p></li>\n</ul>\n\n<p>Where should I start looking for a job? What should I pay attention to? Any tips on what to expect? I&#39;ve seen many Senior postings but none for entry level (I guess that&#39;s me?)</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1d8bgdh', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='RedBlueWhiteBlack'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d8bgdh/im_about_to_quit_my_job_and_start_my_journey_on/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d8bgdh/im_about_to_quit_my_job_and_start_my_journey_on/', 'subreddit_subscribers': 188236, 'created_utc': 1717543537.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.722+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm a novice in DE. I would like the best tips in updating my s3 bucket when my data has an update operation.\nI'm currently fetching data from an HR sql server. Updates on the table I am on can be changing the person's name and if they are an active employee or not etc.\n\nI was thinking of pulling the data that's already there, then comparing the rows (old data that was in s3) with the new rows coming from the sql server. I'd probably use chunking since I'd be comparing all the rows in that table.\n\nI feel like there could be a better way to do this. I was going to use pyspark to perform the above.\n\nPlease advice on the best way to achieve de-duplication", 'author_fullname': 't2_fpzrbnk1t', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'CRUD operations', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d805qf', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717515339.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m a novice in DE. I would like the best tips in updating my s3 bucket when my data has an update operation.\nI&#39;m currently fetching data from an HR sql server. Updates on the table I am on can be changing the person&#39;s name and if they are an active employee or not etc.</p>\n\n<p>I was thinking of pulling the data that&#39;s already there, then comparing the rows (old data that was in s3) with the new rows coming from the sql server. I&#39;d probably use chunking since I&#39;d be comparing all the rows in that table.</p>\n\n<p>I feel like there could be a better way to do this. I was going to use pyspark to perform the above.</p>\n\n<p>Please advice on the best way to achieve de-duplication</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1d805qf', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='RainbowMosaic'), 'discussion_type': None, 'num_comments': 8, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d805qf/crud_operations/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d805qf/crud_operations/', 'subreddit_subscribers': 188236, 'created_utc': 1717515339.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.723+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hello, I've been working as a data engineer for quite a while and want to start learning Spark (Python). Can anyone recommend where to begin, what learning path to take, what sort of practical projects are best, and if there are any good Udemy or other courses?", 'author_fullname': 't2_im18zolz5', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Question: Spark Learning Path ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d8ihpb', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717565974.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello, I&#39;ve been working as a data engineer for quite a while and want to start learning Spark (Python). Can anyone recommend where to begin, what learning path to take, what sort of practical projects are best, and if there are any good Udemy or other courses?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1d8ihpb', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Square-Brick-8727'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d8ihpb/question_spark_learning_path/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d8ihpb/question_spark_learning_path/', 'subreddit_subscribers': 188236, 'created_utc': 1717565974.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.723+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I am building a DW for storing public government data that is updated once a year at most, so my time granularity is only a year, moreover most common examples of columns to put on a dimension table (holiday, Fiscal quarter...) dont apply to my case.\n\n\nIn this cenario i wonder if i really need a date dimension or if i can just store the year on the fact table and if i need to do queries i just open the table and apply filters on the date.\n\n\nI have tried to think about useful columns for a date dimension in my case but i couldn't think of any hard-sells\n\nMy only  idea for this was:\n\n1) is_national_census_year: as my country does national census every 10 or so years but that has no bearing or apparent analytical relevance to other points of data collected (most are independent from the census and census data is identified by another means)\n\n\nWhat do you guys think?\n", 'author_fullname': 't2_7xe340s7', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Date dimension for a Data Warehouse when my date granularity is only years', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d8f1v5', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717554139.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I am building a DW for storing public government data that is updated once a year at most, so my time granularity is only a year, moreover most common examples of columns to put on a dimension table (holiday, Fiscal quarter...) dont apply to my case.</p>\n\n<p>In this cenario i wonder if i really need a date dimension or if i can just store the year on the fact table and if i need to do queries i just open the table and apply filters on the date.</p>\n\n<p>I have tried to think about useful columns for a date dimension in my case but i couldn&#39;t think of any hard-sells</p>\n\n<p>My only  idea for this was:</p>\n\n<p>1) is_national_census_year: as my country does national census every 10 or so years but that has no bearing or apparent analytical relevance to other points of data collected (most are independent from the census and census data is identified by another means)</p>\n\n<p>What do you guys think?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1d8f1v5', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='SnooPineapples7791'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d8f1v5/date_dimension_for_a_data_warehouse_when_my_date/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d8f1v5/date_dimension_for_a_data_warehouse_when_my_date/', 'subreddit_subscribers': 188236, 'created_utc': 1717554139.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.724+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_4xe2pbqe', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Bi-directional sync between Databricks Unity Catalog and Microsoft Purview', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 70, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d8auil', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/1ZTTG7nOT08x3_VpsTr1l4eP02ACV1RQTVU_TnYfGqc.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1717541936.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'medium.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://medium.com/@davegeyer/a-practical-guide-to-programmatic-data-management-using-azure-databricks-unity-catalog-and-79452911f2f5', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/2E5i0yiCPG7WrCsHfssAAiIOAxVpJqgD41S6C9wE-_I.jpg?auto=webp&s=77c29ef1a48c5c7ba5dee017422560c37948837e', 'width': 1070, 'height': 536}, 'resolutions': [{'url': 'https://external-preview.redd.it/2E5i0yiCPG7WrCsHfssAAiIOAxVpJqgD41S6C9wE-_I.jpg?width=108&crop=smart&auto=webp&s=dbfd7a5492a267fd833891062815074944aec655', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/2E5i0yiCPG7WrCsHfssAAiIOAxVpJqgD41S6C9wE-_I.jpg?width=216&crop=smart&auto=webp&s=8d6f0a9c69c938e2b0cc6f5f6f7562d779f8ee5c', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/2E5i0yiCPG7WrCsHfssAAiIOAxVpJqgD41S6C9wE-_I.jpg?width=320&crop=smart&auto=webp&s=abb8d97fc0b85e81631570bf4e3d19208ec6465c', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/2E5i0yiCPG7WrCsHfssAAiIOAxVpJqgD41S6C9wE-_I.jpg?width=640&crop=smart&auto=webp&s=bfd0f07f1605fd1106088f1778ed852d86c44808', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/2E5i0yiCPG7WrCsHfssAAiIOAxVpJqgD41S6C9wE-_I.jpg?width=960&crop=smart&auto=webp&s=baad26d00d73dd8bb6a6a787d2628dbe47e65d61', 'width': 960, 'height': 480}], 'variants': {}, 'id': 'brzIFSZjOopFnnQMgmO5QBBxXEfdi-ejDg9CwhnKgag'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1d8auil', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Remote-Distribution1'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d8auil/bidirectional_sync_between_databricks_unity/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://medium.com/@davegeyer/a-practical-guide-to-programmatic-data-management-using-azure-databricks-unity-catalog-and-79452911f2f5', 'subreddit_subscribers': 188236, 'created_utc': 1717541936.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.724+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_dqm6u', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Polars news: Faster CSV writer, dead expr elimination optimization, hiring engineers.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 72, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1d8md9b', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.86, 'author_flair_background_color': None, 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/dYlQSYApXqaK_qtwYPzC_7Rc9zfrYrOZMV5CEx_wFsE.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1717582314.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'pola.rs', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://pola.rs/posts/polars-in-aggregate-jun24/', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/km18DDg4kDOH29UCKit--cWd5c2r_ZQXXHGp7kDSB04.jpg?auto=webp&s=9b6e10d7f7ab82a5576dfb157229747e8b8cb62c', 'width': 1200, 'height': 620}, 'resolutions': [{'url': 'https://external-preview.redd.it/km18DDg4kDOH29UCKit--cWd5c2r_ZQXXHGp7kDSB04.jpg?width=108&crop=smart&auto=webp&s=a13c985a41ce7d4ae398439d9fc9491062d1f4a3', 'width': 108, 'height': 55}, {'url': 'https://external-preview.redd.it/km18DDg4kDOH29UCKit--cWd5c2r_ZQXXHGp7kDSB04.jpg?width=216&crop=smart&auto=webp&s=caa14a5df8a6493028156e1acaee30d11ff1125b', 'width': 216, 'height': 111}, {'url': 'https://external-preview.redd.it/km18DDg4kDOH29UCKit--cWd5c2r_ZQXXHGp7kDSB04.jpg?width=320&crop=smart&auto=webp&s=b6d5a73c03763160d2000779610e0edcfa258b6a', 'width': 320, 'height': 165}, {'url': 'https://external-preview.redd.it/km18DDg4kDOH29UCKit--cWd5c2r_ZQXXHGp7kDSB04.jpg?width=640&crop=smart&auto=webp&s=f09e4d32f02ef22273a788ac224e9162d1b34437', 'width': 640, 'height': 330}, {'url': 'https://external-preview.redd.it/km18DDg4kDOH29UCKit--cWd5c2r_ZQXXHGp7kDSB04.jpg?width=960&crop=smart&auto=webp&s=88ea71b109274a3f47ee9d0a9ec2dff360e176f1', 'width': 960, 'height': 496}, {'url': 'https://external-preview.redd.it/km18DDg4kDOH29UCKit--cWd5c2r_ZQXXHGp7kDSB04.jpg?width=1080&crop=smart&auto=webp&s=db840f0a7bfc14d0323eefde041b8c5bf46bde83', 'width': 1080, 'height': 558}], 'variants': {}, 'id': 'IrOR8miS2fcelbmj6Bfmv_ZfXA56BuYDFnmxXWQxgpU'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1d8md9b', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='commandlineluser'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d8md9b/polars_news_faster_csv_writer_dead_expr/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://pola.rs/posts/polars-in-aggregate-jun24/', 'subreddit_subscribers': 188236, 'created_utc': 1717582314.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.725+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_1c6f704', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Pipeline in a Container: Docker Essentials for Data Engineers', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 70, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d8lxyg', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': 'transparent', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': '9ecf3c88-e787-11ed-957e-de1616aeae13', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/Z_NTfk2AzIV1cd7UMHvHlKvdkmAn_2xby27vSDKv_a4.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1717580576.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'open.substack.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://open.substack.com/pub/datagibberish/p/docker-essentials?r=odlo3&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/EIe8ohV2335o8j5ivShaZupbgJymAeqA3E6LxJ8Yp0Y.jpg?auto=webp&s=ff7930b2b5ff636cfffd38c9ec97308e7ec6fe7a', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/EIe8ohV2335o8j5ivShaZupbgJymAeqA3E6LxJ8Yp0Y.jpg?width=108&crop=smart&auto=webp&s=0a4d004e92fc70bf03a716a0455571b31b3ff007', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/EIe8ohV2335o8j5ivShaZupbgJymAeqA3E6LxJ8Yp0Y.jpg?width=216&crop=smart&auto=webp&s=ded3ff90fb42b7bebb35650f3d03c2b07a81dfb8', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/EIe8ohV2335o8j5ivShaZupbgJymAeqA3E6LxJ8Yp0Y.jpg?width=320&crop=smart&auto=webp&s=f30c2f332aedacbcb12f313bdad4050fca939d1c', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/EIe8ohV2335o8j5ivShaZupbgJymAeqA3E6LxJ8Yp0Y.jpg?width=640&crop=smart&auto=webp&s=43ca00ec0ec638c8fe26e1d53ea23edf9f848323', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/EIe8ohV2335o8j5ivShaZupbgJymAeqA3E6LxJ8Yp0Y.jpg?width=960&crop=smart&auto=webp&s=7892875eb168471c002845ed7e854ea8ef44f5b5', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/EIe8ohV2335o8j5ivShaZupbgJymAeqA3E6LxJ8Yp0Y.jpg?width=1080&crop=smart&auto=webp&s=4a8b42867f00847a9af0d61dd4100e79d22fa8f1', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'y41V5ejVMS8bXkZq8P7BzL8se3CCyeh35AWAXb8Zj-0'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Data Engineering Manager', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1d8lxyg', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ivanovyordan'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/dataengineering/comments/1d8lxyg/pipeline_in_a_container_docker_essentials_for/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://open.substack.com/pub/datagibberish/p/docker-essentials?r=odlo3&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true', 'subreddit_subscribers': 188236, 'created_utc': 1717580576.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.725+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi all, I have an opportunity for work to pay for the exam. Hoping to get your thoughts on the study load to prep for the exam and the exam itself?\n\nAlso, would it be worth it if work didn’t pay? I see it needs to be renewed every 12 months.\n\nCheers', 'author_fullname': 't2_5iincanh', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Microsoft Cert: Azure Data Engineering Associate', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d8cp7h', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717547034.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi all, I have an opportunity for work to pay for the exam. Hoping to get your thoughts on the study load to prep for the exam and the exam itself?</p>\n\n<p>Also, would it be worth it if work didn’t pay? I see it needs to be renewed every 12 months.</p>\n\n<p>Cheers</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1d8cp7h', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Willing_Excuse1652'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d8cp7h/microsoft_cert_azure_data_engineering_associate/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d8cp7h/microsoft_cert_azure_data_engineering_associate/', 'subreddit_subscribers': 188236, 'created_utc': 1717547034.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.725+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hello to everyone we are working in a recommendation system which requires user history clicks, so data needs a heavy processing before going to the model, we work with AWS and the solution we have so far is \n\n1. Data replicated in batch from DB production table in Athena (dump in s3)\n2. Views created in Athena to make staging tables (reads and writes backs to s3)\n3. final pre-processing query and load data to pandas\n4. pandas df merges with some other data not in Athena tables (heavy processing) then write back the final data to s3\n5. Model uses data\n\nMy main concerns are the scalability of the pipeline, we are POCing with 300 users and batch every 4hours which works fine but the scale can go up 500k and frequency might increase, I would like your thoughts on this :) thank you so much for commenting ', 'author_fullname': 't2_42yrzhea', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Model training data', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d7yqma', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717511827.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello to everyone we are working in a recommendation system which requires user history clicks, so data needs a heavy processing before going to the model, we work with AWS and the solution we have so far is </p>\n\n<ol>\n<li>Data replicated in batch from DB production table in Athena (dump in s3)</li>\n<li>Views created in Athena to make staging tables (reads and writes backs to s3)</li>\n<li>final pre-processing query and load data to pandas</li>\n<li>pandas df merges with some other data not in Athena tables (heavy processing) then write back the final data to s3</li>\n<li>Model uses data</li>\n</ol>\n\n<p>My main concerns are the scalability of the pipeline, we are POCing with 300 users and batch every 4hours which works fine but the scale can go up 500k and frequency might increase, I would like your thoughts on this :) thank you so much for commenting </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1d7yqma', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='josejo9423'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d7yqma/model_training_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d7yqma/model_training_data/', 'subreddit_subscribers': 188236, 'created_utc': 1717511827.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.726+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I have a problem where I am consolidating about 100 CSV files into one file but some of the CSV files have dates in day/month/year and others have it in month/day/year format. Is there a way to deal with these locales in the different files within a transformation pipeline, where I don't have to manually touch each file individually? Is there a way to detect and then switch locales using a function?\n\nAny help would be greatly appreciated, I am trying to avoid going into each CSV file to change it manually and just want to batch process.", 'author_fullname': 't2_16ojwa', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Dealing with different date locales in multiple CSV files', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d7yd98', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717510879.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I have a problem where I am consolidating about 100 CSV files into one file but some of the CSV files have dates in day/month/year and others have it in month/day/year format. Is there a way to deal with these locales in the different files within a transformation pipeline, where I don&#39;t have to manually touch each file individually? Is there a way to detect and then switch locales using a function?</p>\n\n<p>Any help would be greatly appreciated, I am trying to avoid going into each CSV file to change it manually and just want to batch process.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1d7yd98', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='bonesclarke84'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d7yd98/dealing_with_different_date_locales_in_multiple/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d7yd98/dealing_with_different_date_locales_in_multiple/', 'subreddit_subscribers': 188236, 'created_utc': 1717510879.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.726+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_4nclic9b', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Identifying Container Image Vulnerabilities with Docker Scout', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 70, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d7vhts', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/D3CpNe8gDp6ctexT2fQkF4uaQmZthyzGKa2EJN9iQe8.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1717502762.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'packagemain.tech', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://packagemain.tech/p/identifying-container-image-vulnerabilities', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/MZkDGf0UzF6KbV0GtIRkZioa0qdE8eSiLP0Q8bUXt_M.jpg?auto=webp&s=080a7094ce3c5db5e2d0f62e1e5066d114542a61', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/MZkDGf0UzF6KbV0GtIRkZioa0qdE8eSiLP0Q8bUXt_M.jpg?width=108&crop=smart&auto=webp&s=90fa4999832b2ed543c6afed3c3558c0fb8d24fe', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/MZkDGf0UzF6KbV0GtIRkZioa0qdE8eSiLP0Q8bUXt_M.jpg?width=216&crop=smart&auto=webp&s=b34df1005eef5623cb1700909c1a5e0303fe892e', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/MZkDGf0UzF6KbV0GtIRkZioa0qdE8eSiLP0Q8bUXt_M.jpg?width=320&crop=smart&auto=webp&s=a925027fbdb2745915dac2440bd0b6f5782b0bc5', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/MZkDGf0UzF6KbV0GtIRkZioa0qdE8eSiLP0Q8bUXt_M.jpg?width=640&crop=smart&auto=webp&s=1a515f641c66d98516b5596cbe41c0564b613492', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/MZkDGf0UzF6KbV0GtIRkZioa0qdE8eSiLP0Q8bUXt_M.jpg?width=960&crop=smart&auto=webp&s=5dad78c094f2f6b4d5e4c7504fab2a1e79bd2ca2', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/MZkDGf0UzF6KbV0GtIRkZioa0qdE8eSiLP0Q8bUXt_M.jpg?width=1080&crop=smart&auto=webp&s=337662b1e68a1822b0e323dd05486a28acf35846', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'Q3Ru0WTrPdx8D0XwfjD_zOABkjClNTKngCliWpeWZIM'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1d7vhts', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='pliutau'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d7vhts/identifying_container_image_vulnerabilities_with/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://packagemain.tech/p/identifying-container-image-vulnerabilities', 'subreddit_subscribers': 188236, 'created_utc': 1717502762.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.727+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi guys,\n\nI need some advice, and I have a feeling many motivated professionals struggle with this too.\n\nI am a highly motivated and disciplined person. I work as a data engineer (2YE), and I mainly use SQL and Databricks for building data pipelines. I specifically specialize in complex data migrations. \n\nI also have just started my own company with a good friend. It's an AI-based application written completely in Python.\n\nI don't have a background in computer science, but Innovation Science and Data science. That's why I'm really proud of myself if I'm able to solve some medium level leetcode questions. \n\nBut my motivation and discipline are going to cost me my mental well-being. Right now I'm on a holiday, and I have this irrational fear of losing my skills if I haven't written any code in 2 days or so. I know this is ridiculous, but I just can't enjoy my holiday and feel like I HAVE TO do some leetcode to maintain my skills.\n\nDoes anyone struggle with this too? What did you do to find some peace? How can I get rid of this absurd, irrational fear. \n\nTldr: how the fuck can I chill out and don't think about coding. ", 'author_fullname': 't2_idmfe2je', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How to just sit back and enjoy', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1d8nkvi', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717586808.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi guys,</p>\n\n<p>I need some advice, and I have a feeling many motivated professionals struggle with this too.</p>\n\n<p>I am a highly motivated and disciplined person. I work as a data engineer (2YE), and I mainly use SQL and Databricks for building data pipelines. I specifically specialize in complex data migrations. </p>\n\n<p>I also have just started my own company with a good friend. It&#39;s an AI-based application written completely in Python.</p>\n\n<p>I don&#39;t have a background in computer science, but Innovation Science and Data science. That&#39;s why I&#39;m really proud of myself if I&#39;m able to solve some medium level leetcode questions. </p>\n\n<p>But my motivation and discipline are going to cost me my mental well-being. Right now I&#39;m on a holiday, and I have this irrational fear of losing my skills if I haven&#39;t written any code in 2 days or so. I know this is ridiculous, but I just can&#39;t enjoy my holiday and feel like I HAVE TO do some leetcode to maintain my skills.</p>\n\n<p>Does anyone struggle with this too? What did you do to find some peace? How can I get rid of this absurd, irrational fear. </p>\n\n<p>Tldr: how the fuck can I chill out and don&#39;t think about coding. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1d8nkvi', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='DarthDatar-4058'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d8nkvi/how_to_just_sit_back_and_enjoy/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d8nkvi/how_to_just_sit_back_and_enjoy/', 'subreddit_subscribers': 188236, 'created_utc': 1717586808.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.727+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I read z order should be done on high cardinality columns. Also, it decreases in efficacity the more you throw columns at it.\n\nBut is z-ordering by multiple low cardinality columns (meaning that the cardinality of the concat of those columns column1\\_\\[...\\]\\_columnN will be high) okay for performance?\n\nAs a rule of thumb, I z order on business keys (e.g. for a stock snapshot: location\\_id, product\\_id, date) as they also are the most likely to be queried against.\n\nWhat would you do differently?', 'author_fullname': 't2_15hhj9', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Delta table Z-order and columnS cardinality ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d8k2wn', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717572525.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I read z order should be done on high cardinality columns. Also, it decreases in efficacity the more you throw columns at it.</p>\n\n<p>But is z-ordering by multiple low cardinality columns (meaning that the cardinality of the concat of those columns column1_[...]_columnN will be high) okay for performance?</p>\n\n<p>As a rule of thumb, I z order on business keys (e.g. for a stock snapshot: location_id, product_id, date) as they also are the most likely to be queried against.</p>\n\n<p>What would you do differently?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1d8k2wn', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='mlobet'), 'discussion_type': None, 'num_comments': 7, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d8k2wn/delta_table_zorder_and_columns_cardinality/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d8k2wn/delta_table_zorder_and_columns_cardinality/', 'subreddit_subscribers': 188236, 'created_utc': 1717572525.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.728+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hello,\n\nWe have a requirement in which there are multiple third party partners where we have our data sitting on their cloud. Few are Azure cloud and some are on AWS. Few are on databases and few residing on files. Now we want those data to bring back to our own cloud account which is on AWS. Want to understand how to approach this requirement and what are the options at hand to do this migration with minimal cost and lesser time. What all information we need to gather for making the migration in an efficient way? Need guidance on this.\n\nWe are trying to get the below details , but with regards to the tools for doing this migration, i am aware of aws DMS for database to database migration. But can you suggest of we should use or explore any other tool for this activity?\n\nTotal volume of data which need to be moved and if the source data is changing?\n\nType of data(files, Datababse, logs)\n\nLocation of the data(Azure/Aws), access credentials, network configs\n\nwhether we should compress before we move?\n\nwhether we should do encryption before we move ?', 'author_fullname': 't2_vvsk9e6n', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Moving data across efficiently', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d8jeil', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717569630.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello,</p>\n\n<p>We have a requirement in which there are multiple third party partners where we have our data sitting on their cloud. Few are Azure cloud and some are on AWS. Few are on databases and few residing on files. Now we want those data to bring back to our own cloud account which is on AWS. Want to understand how to approach this requirement and what are the options at hand to do this migration with minimal cost and lesser time. What all information we need to gather for making the migration in an efficient way? Need guidance on this.</p>\n\n<p>We are trying to get the below details , but with regards to the tools for doing this migration, i am aware of aws DMS for database to database migration. But can you suggest of we should use or explore any other tool for this activity?</p>\n\n<p>Total volume of data which need to be moved and if the source data is changing?</p>\n\n<p>Type of data(files, Datababse, logs)</p>\n\n<p>Location of the data(Azure/Aws), access credentials, network configs</p>\n\n<p>whether we should compress before we move?</p>\n\n<p>whether we should do encryption before we move ?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1d8jeil', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ConsiderationLazy956'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d8jeil/moving_data_across_efficiently/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d8jeil/moving_data_across_efficiently/', 'subreddit_subscribers': 188236, 'created_utc': 1717569630.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.729+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "So I am refreshing the basics of Apache Spark and I am confused. I have looked at various sources and I am getting different answers. I am trying to learn about Apache Spark Components. Which one of these  is correct? What is the right term for these? \n\n**Spark Components are**: \n\n* Spark Core\n\n* Spark SQL \n\n* Spark Streaming \n\n* MLLib \n\n* GraphX \n\nTo me the above are just libraries for different use cases.\n\nI have also seen some articles talk about the following being the components of Spark\n\n* Cluster Manager \n\n* Task \n\n* Job \n\n* Executor \n\nWhat's the right terminology\n\nThanks for your help\n", 'author_fullname': 't2_1x6qmmve', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Confused about Spark Components ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d8goix', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.72, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717559446.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>So I am refreshing the basics of Apache Spark and I am confused. I have looked at various sources and I am getting different answers. I am trying to learn about Apache Spark Components. Which one of these  is correct? What is the right term for these? </p>\n\n<p><strong>Spark Components are</strong>: </p>\n\n<ul>\n<li><p>Spark Core</p></li>\n<li><p>Spark SQL </p></li>\n<li><p>Spark Streaming </p></li>\n<li><p>MLLib </p></li>\n<li><p>GraphX </p></li>\n</ul>\n\n<p>To me the above are just libraries for different use cases.</p>\n\n<p>I have also seen some articles talk about the following being the components of Spark</p>\n\n<ul>\n<li><p>Cluster Manager </p></li>\n<li><p>Task </p></li>\n<li><p>Job </p></li>\n<li><p>Executor </p></li>\n</ul>\n\n<p>What&#39;s the right terminology</p>\n\n<p>Thanks for your help</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1d8goix', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='notechmajor'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d8goix/confused_about_spark_components/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d8goix/confused_about_spark_components/', 'subreddit_subscribers': 188236, 'created_utc': 1717559446.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.729+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_lnwagoki', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Apache Iceberg 101 - Learning Apache Iceberg', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d88h79', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'default', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1717535898.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'dremio.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.dremio.com/blog/apache-iceberg-101-your-guide-to-learning-apache-iceberg-concepts-and-practices/', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1d88h79', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='AMDataLake'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d88h79/apache_iceberg_101_learning_apache_iceberg/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.dremio.com/blog/apache-iceberg-101-your-guide-to-learning-apache-iceberg-concepts-and-practices/', 'subreddit_subscribers': 188236, 'created_utc': 1717535898.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.730+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Wondering what the Iceberg fans are gonna think? It used to be the argument that "it\'s more open than Delta". Maybe not the case anymore?', 'author_fullname': 't2_gbjfq', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Big development in the open table format space', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d8152u', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717517765.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Wondering what the Iceberg fans are gonna think? It used to be the argument that &quot;it&#39;s more open than Delta&quot;. Maybe not the case anymore?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1d8152u', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='britishbanana'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d8152u/big_development_in_the_open_table_format_space/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d8152u/big_development_in_the_open_table_format_space/', 'subreddit_subscribers': 188236, 'created_utc': 1717517765.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.731+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi Guys\n\n  \nI've been googling and not found anything promising so I assume it's not possible but wanted to double check before going with a different method.\n\nIs there anyway I can use the MERG INTO command from Databricks to merge data directly into an Azure SQL DB table?\n\nI've got Unity Catalog setup on my workspace.", 'author_fullname': 't2_56o0g58i', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Azure Databricks MERGE INTO Azure SQL DB table with existing data', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d7w6qp', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717504924.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi Guys</p>\n\n<p>I&#39;ve been googling and not found anything promising so I assume it&#39;s not possible but wanted to double check before going with a different method.</p>\n\n<p>Is there anyway I can use the MERG INTO command from Databricks to merge data directly into an Azure SQL DB table?</p>\n\n<p>I&#39;ve got Unity Catalog setup on my workspace.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1d7w6qp', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='IG-55'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d7w6qp/azure_databricks_merge_into_azure_sql_db_table/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d7w6qp/azure_databricks_merge_into_azure_sql_db_table/', 'subreddit_subscribers': 188236, 'created_utc': 1717504924.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.731+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm an experienced data analyst looking to transition to data engineering. Looking for recommendations on courses, books, content, etc too learn DE and make the transition.\n\nBackground: 15 years working with data - SQL, SAS, and relational databases. Business Intelligence tools like Tableau, Power BI & MicroStrstegy. Also, some light experience in Python with the popular data & machine learning packages.  I have excellent soft skills.", 'author_fullname': 't2_rh3kf42i', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Transitioning to Date Engineering', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1d8n3rh', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717585131.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m an experienced data analyst looking to transition to data engineering. Looking for recommendations on courses, books, content, etc too learn DE and make the transition.</p>\n\n<p>Background: 15 years working with data - SQL, SAS, and relational databases. Business Intelligence tools like Tableau, Power BI &amp; MicroStrstegy. Also, some light experience in Python with the popular data &amp; machine learning packages.  I have excellent soft skills.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1d8n3rh', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ddetts'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d8n3rh/transitioning_to_date_engineering/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d8n3rh/transitioning_to_date_engineering/', 'subreddit_subscribers': 188236, 'created_utc': 1717585131.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.731+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I am recently learning Databricks right now, and I feel it's very cumbersome and heavy, I don't know what's  the best use case for that. When do I need it? Why do I need it? \n\n  \nIs there any paid tools/service overrated in Data Engineering and Data Science, expensive but does not solve the problem.?", 'author_fullname': 't2_v4sp85phd', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Is there any paid tools/service overrated in Data Engineering and Data Science, expensive but does not solve the problem. ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d8m1yq', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717581049.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I am recently learning Databricks right now, and I feel it&#39;s very cumbersome and heavy, I don&#39;t know what&#39;s  the best use case for that. When do I need it? Why do I need it? </p>\n\n<p>Is there any paid tools/service overrated in Data Engineering and Data Science, expensive but does not solve the problem.?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1d8m1yq', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Realistic_Wave2856'), 'discussion_type': None, 'num_comments': 8, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d8m1yq/is_there_any_paid_toolsservice_overrated_in_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d8m1yq/is_there_any_paid_toolsservice_overrated_in_data/', 'subreddit_subscribers': 188236, 'created_utc': 1717581049.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.732+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I want to avoid using check cols strategy for my snapshots for performance reasons however when using timestamp strategy it forces the dbt_valid_from to be the updated_date which is not what I want, I want the dbt_valid_from to be set to the date it was run and inserted as it does when using check cols.\n\nThe reason for this is to improve performance of incremental loads in lower layers.\n\nJust wondering if it's possible to configure snapshots or do you have to manually override the snapshot macros or create your own macros.", 'author_fullname': 't2_g6zwu97i8', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Dbt issue with snapshots and dates', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d8idqm', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717565537.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I want to avoid using check cols strategy for my snapshots for performance reasons however when using timestamp strategy it forces the dbt_valid_from to be the updated_date which is not what I want, I want the dbt_valid_from to be set to the date it was run and inserted as it does when using check cols.</p>\n\n<p>The reason for this is to improve performance of incremental loads in lower layers.</p>\n\n<p>Just wondering if it&#39;s possible to configure snapshots or do you have to manually override the snapshot macros or create your own macros.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1d8idqm', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Bootlegcrunch'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d8idqm/dbt_issue_with_snapshots_and_dates/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d8idqm/dbt_issue_with_snapshots_and_dates/', 'subreddit_subscribers': 188236, 'created_utc': 1717565537.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.732+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi everyone,\nI need some advice. I have been managing a team of 6 for past 10 months, but my official job title is Senior Data Engineer. My dilemma is, when applying for jobs externally, should I mention my official title on Linkedn and my resumes, or should I reframe it to Data Engineering Manager? I have applied for managerial roles with my official job title but have received several rejections. I have total 10 years of industry experience in data roles.\nAny valuable suggestions or strategies would be greatly appreciated.', 'author_fullname': 't2_hx28p47x', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Seeking Advice: Should I Reframe My Job Title on resumes and LinkedIn?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d85jmj', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717528703.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi everyone,\nI need some advice. I have been managing a team of 6 for past 10 months, but my official job title is Senior Data Engineer. My dilemma is, when applying for jobs externally, should I mention my official title on Linkedn and my resumes, or should I reframe it to Data Engineering Manager? I have applied for managerial roles with my official job title but have received several rejections. I have total 10 years of industry experience in data roles.\nAny valuable suggestions or strategies would be greatly appreciated.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1d85jmj', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Classic-Net9806'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d85jmj/seeking_advice_should_i_reframe_my_job_title_on/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d85jmj/seeking_advice_should_i_reframe_my_job_title_on/', 'subreddit_subscribers': 188236, 'created_utc': 1717528703.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.733+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff82f66910>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hello!\n\nI am trying to help a friend with some data engineering.\n\nProblem: My friend has a company that helps clients with agriculture engineering, basically, for each client he ends up having a lot of excel files that have data on agriculture and some shapefiles.\n\nNeed: I proposed to create a dashboard that reports things using the shapefiles to show maps.\n\nThis obviously needs to standardize how data is stored and put it in a proper db, but I keep wondering, would this be solved by giving him a structure to follow when storing data of clients? Or does this need a software with an UI that stores data in a db?\nIs moving excel files and shapefiles to a proper db valuable? Or am I just adding an extra step for my friend?\n\nI'm a bit lost here trying to create a good solution, maybe I'm overthinking it, how would you go about it?", 'author_fullname': 't2_b9f51wwn', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'DE or SWD? How would you solve this? ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1d8fnmu', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1717556063.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello!</p>\n\n<p>I am trying to help a friend with some data engineering.</p>\n\n<p>Problem: My friend has a company that helps clients with agriculture engineering, basically, for each client he ends up having a lot of excel files that have data on agriculture and some shapefiles.</p>\n\n<p>Need: I proposed to create a dashboard that reports things using the shapefiles to show maps.</p>\n\n<p>This obviously needs to standardize how data is stored and put it in a proper db, but I keep wondering, would this be solved by giving him a structure to follow when storing data of clients? Or does this need a software with an UI that stores data in a db?\nIs moving excel files and shapefiles to a proper db valuable? Or am I just adding an extra step for my friend?</p>\n\n<p>I&#39;m a bit lost here trying to create a good solution, maybe I&#39;m overthinking it, how would you go about it?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1d8fnmu', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='gabrielndluna'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1d8fnmu/de_or_swd_how_would_you_solve_this/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1d8fnmu/de_or_swd_how_would_you_solve_this/', 'subreddit_subscribers': 188236, 'created_utc': 1717556063.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-06-05T12:09:36.733+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-05T12:09:36.734+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-05T12:09:36.741+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=reddit_etl_pipeline, task_id=reddit_extraction, run_id=manual__2024-06-05T12:09:35.121137+00:00, execution_date=20240605T120935, start_date=20240605T120935, end_date=20240605T120936
[2024-06-05T12:09:36.784+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-05T12:09:36.794+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-05T12:09:36.795+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
